---
title: Sprint3. Creaci√≥n de modelos
description: Comprender qu√© es un sesgo en la IA y planificar un experimento para crearlo.
---

En este sprint, nos adentramos en un desaf√≠o clave de la IA: los **sesgos algor√≠tmicos**. Analizaremos c√≥mo estos prejuicios invisibles pueden sabotear el √©xito de una organizaci√≥n y, lo que es m√°s importante, **perpetuar la discriminaci√≥n**, impidiendo que todas las personas participen en la sociedad en igualdad de condiciones.

No solo hablaremos de sesgos, sino que tambi√©n **crearemos nuestro propio modelo sesgado** con Teachable Machine para experimentar sus efectos y debatir sobre sus implicaciones √©ticas y sociales.

## Sesi√≥n 1: La Teor√≠a del Sesgo

Hemos hablado de la importancia de los datos en la IA, ya que un modelo de IA aprende a partir de los datos de entrenamiento que se le proporcionan. Hablamos de los problemas de sobreajuste (overfitting) y de subajuste (underfitting). Recordemos:

- **Sobreajuste (overfitting)**: ocurre cuando un modelo aprende demasiado bien los datos de entrenamiento, incluyendo el ruido y las anomal√≠as, lo que resulta en un rendimiento deficiente en datos nuevos.
- **Subajuste (underfitting)**: sucede cuando un modelo no captura adecuadamente los patrones en los datos de entrenamiento, lo que lleva a un rendimiento deficiente tanto en los datos de entrenamiento como en los nuevos.

Sin embargo, hay otro gran problema, la discriminaci√≥n silenciosa que est√° incrustada en las monta√±as de datos con los que se entrenan la IA. Pongamos como ejemplo que los grandes modelos de IA generativa como ChatGPT, Gemini.., que han sido entrenados a partir de grandes v√≥lumenes de datos procedentes de Internet. Estos modelos aprenden patrones ling√º√≠sticos y conceptuales c√≥mo se combinan las palabras, qu√© significan las frases, y c√≥mo se expresan las ideas.

Sin embargo, **no distinguen entre los correcto y lo incorrecto**. Si los datos de entrenamiento contienen **estereotipos, prejuicios o desigualdades sociales**, el modelo puede heredar y reproducir esos sesgos en sus respuestas.

Por ejemplo, si los textos de entrenamiento contiene como estereotipo de g√©nero, los hombres son programadores y las mujeres son enfermeras, el modelo puede generar respuestas que refuercen estos estereotipos, como asociar autom√°ticamente ciertos roles o profesiones con un g√©nero espec√≠fico.

!!! example "Caso de estudio"

    **Grok** es el chatbot de *xAI*, empresa de inteligencia artificial creada por *Elon Musk*, y que est√° integrado en la plataforma X.

    Grok fue entrenado entre otras fuentes con datos de X, donde los usuarios publican mensajes que pueden contener **lenguaje ofensivo, desinformaci√≥n y sesgos**. Adem√°s, Grok fue instruido para **no evitar hacer afirmaciones pol√≠ticamente incorrectas, siempre que est√©n bien fundamentadas**, sin saber qu√© significa "bien fundamentadas". Debido a esto, Grok ha generado respuestas que han sido criticadas por ser **sexistas, racistas y promover teor√≠as de conspiraci√≥n**. Ejemplos:

    | üß© **Caso** | üí¨ **Qu√© dijo / hizo Grok** | ‚ö†Ô∏è **Qu√© muestra de sesgo o error** |
    |--------------|-----------------------------|-------------------------------------|
    | **‚ÄúWhite genocide‚Äù fuera de contexto** [NyPost](https://nypost.com/2025/05/15/business/elon-musks-grok-ai-bot-says-it-appears-i-was-instructed-to-discuss-white-genocide/?utm_source=chatgpt.com)| Grok comenz√≥ a introducir referencias al concepto de ‚Äúwhite genocide‚Äù en Sud√°frica en respuestas que no ten√≠an nada que ver con ese tema, como preguntas sobre cambio de nombre de HBO o luchas de wrestling. | Introducci√≥n de teor√≠as conspirativas sin evidencia. Sesgo hacia narrativas pol√≠ticas extremas. |
    | **Lenguaje ofensivo y ataques pol√≠ticos** [The Guardian](https://www.theguardian.com/technology/2025/jul/08/musks-grok-ai-bot-generates-expletive-laden-rants-to-questions-on-polish-politics?utm_source=chatgpt.com) | En Polonia, Grok llam√≥ al primer ministro ‚Äúfucking traitor‚Äù, lo acus√≥ de traici√≥n hacia Alemania y la UE, entre otros insultos. | Tono agresivo y poco neutral. Polarizaci√≥n ideol√≥gica y sesgo emocional. |
    | **Uso de estad√≠sticas inventadas** [Reddit](https://www.reddit.com/r/grok/comments/1lz3ebg/grok_cites_nonexisting_data_supporting_antiwoke/?utm_source=chatgpt.com)| Grok afirm√≥ falsamente que una mayor√≠a de estadounidenses ve√≠a lo ‚Äú*woke*‚Äù de manera negativa, citando encuestas inexistentes. | Fabricaci√≥n de datos (‚Äúalucinaci√≥n‚Äù) para reforzar una idea, fomentando desinformaci√≥n. |

    En la actualidad, *xAI* ha mejorado Grok para que evite temas controvertidos y no genere respuestas ofensivas o inapropiadas, pero el caso de Grok ilustra c√≥mo los sesgos en los datos de entrenamiento pueden llevar a resultados problem√°ticos en los modelos de IA.

### 1.1 Concepto de sesgos

La RAE define sesgado/da como relacionado con informaci√≥n 'tendenciosa' y √©sta a su vez como "que manifiesta parcialidad, obedeciendo a una tendencia o idea determinadas". 

En el contexto de la IA, existen diferentes definiciones de sesgo:

- La Organizaci√≥n Internacional de Normalizaci√≥n (**ISO**) define sesgo en la IA como "**el grado en que un valor de referencia se desv√≠a de la verdad**. 
- A su vez, en los est√°ndares ISO/IEC 22989 se define a los sesgos como la "**diferencia sistem√°tica de trato de determinados objetos, personas o grupos en comparaci√≥n con otros**"

Cuando vemos un resultado inexacto puede ser que este se derive bien de un sesgo o bien de un error. **Los sesgos en Inteligencia Artificial no son simples errores aleatorios, sino que obedecen a patrones sistem√°ticos**.

Como dice el NIST:

"**El sesgo es un efecto que priva a un resultado estad√≠stico de representatividad al distorsionarlo, a diferencia de un error aleatorio, que puede distorsionarlo en cualquier ocasi√≥n, pero se equilibra en promedio**".


### 1.2 Discriminaci√≥n

Igual que no debemos de confundir los errores con los sesgos tampoco debemos de confundir los sesgos con la discriminaci√≥n, que es una de las posibles consecuencias de los sesgos.

La desviaci√≥n de la verdad que se produce en los sesgos puede contribuir a resultados diversos: **discriminatorios, ser neutra, o incluso puede ser beneficiosa**.

Como hemos expuesto antes, en el caso del sistema IA entrenado con estereotipos de g√©nero (por ejemplo, la b√∫squeda de un perfil que ha desempe√±ado hist√≥ricamente mayormente un sexo) utilizar√° ese sesgo en la fase de inferencia y, por tanto, producir√° **resultados discriminatorios hacia ese sexo** puesto que el hecho de que hist√≥ricamente hayan desempe√±ado ese rol en un sexo no significa que lo vayan o deban desempe√±ar mejor en el futuro las personas de ese sexo. 

Pero, si el sistema de IA se ha entrenado con perfiles muy cualificados, el resultado (desde esa √≥ptica) puede ser positivo pues ofrece a los candidatos m√°s capacitados. En este caso, el **sesgo es beneficioso (positivo)**

Un ejemplo de **sesgo neutro** ser√≠a una IA que muestra una preferencia por candidatos que han incluido la palabra "organigrama" en su curr√≠culum. Es un sesgo porque el sistema est√° dando un valor predictivo a una palabra espec√≠fica que no necesariamente se correlaciona con el √©xito en el puesto. La IA, al analizar los curr√≠culums de empleados actuales que tienen buen rendimiento, podr√≠a haber "aprendido" que muchos de ellos usaron esa palabra. Por lo tanto, asocia su presencia con un buen perfil.

Este sesgo es neutro porque la probabilidad de usar la palabra "organigrama" no est√° sistem√°ticamente ligada a ning√∫n grupo demogr√°fico protegido (como g√©nero, etnia, edad, etc.). Simplemente refleja una cierta jerga o estilo de redacci√≥n de curr√≠culums que es aleatorio entre la poblaci√≥n de candidatos.

### 1.3 Exclusi√≥n

Un concepto relacionado con la discriminaci√≥n, pero diferente es el de exclusi√≥n. 

Un ejemplo de **sesgo discriminatorio y excluyente en una IA** es un sistema de selecci√≥n de personal entrenado con datos hist√≥ricos que penaliza los curr√≠culums que incluyen un vac√≠o laboral por "permiso de maternidad".

Este sesgo es discriminatorio porque se dirige y perjudica injustamente a un grupo espec√≠fico: las mujeres. Aunque un hombre puede tomar un permiso de paternidad, hist√≥ricamente est√° m√°s ligado a las mujeres. Al aprender de datos pasados donde las pausas en la carrera (especialmente por motivos familiares) eran vistas negativamente o eran menos comunes en los perfiles contratados (que hist√≥ricamente eran hombres), la IA asocia el "permiso de maternidad" con un menor rendimiento o compromiso, penalizando a las candidatas por una raz√≥n directamente ligada a su g√©nero.

A la vez, es excluyente porque el resultado pr√°ctico de esta discriminaci√≥n es que a un grupo de candidatas perfectamente cualificadas se les niega sistem√°ticamente la oportunidad de avanzar en el proceso de selecci√≥n.

Por otro lado, un ejemplo de **sesgo discriminatorio pero no excluyente** ser√≠a un algoritmo de IA de publicidad de pr√©stamos que muestra anuncios con tasas de inter√©s m√°s altas a usuarios que viven en c√≥digos postales de bajos ingresos.

El sesgo es discriminatorio porque se dirige y perjudica a un grupo de personas bas√°ndose en un factor socioecon√≥mico indirecto: su lugar de residencia. El algoritmo ha aprendido de los datos que ciertos c√≥digos postales se correlacionan con un mayor riesgo crediticio o con una mayor probabilidad de aceptar condiciones menos favorables. Al mostrar sistem√°ticamente peores ofertas a este grupo, la IA perpet√∫a y amplifica una desventaja econ√≥mica basada en la geograf√≠a, lo cual es una forma de discriminaci√≥n.

Este sesgo no es excluyente porque no niega por completo el acceso al producto o servicio. Los residentes de estas √°reas todav√≠a ven los anuncios y tienen la oportunidad de solicitar el pr√©stamo. No se les bloquea ni se les impide participar. Sin embargo, se les ofrece la oportunidad en condiciones peores que a otros grupos, lo que constituye una discriminaci√≥n en la calidad del acceso, pero no en el acceso en s√≠ mismo.

Por tanto, todos los errores no son sesgos y todos los sesgos no son negativos ni todos los sesgos negativos son discriminatorios, ni toda discriminaci√≥n produce exclusi√≥n.

### 1.4 Equidad

Finalmente hay otro concepto diferente y relacionado que es la equidad o justicia. 

En el contexto de la IA, la injusticia puede entenderse como el "trato diferencial injustificado que beneficia preferentemente a ciertos grupos sobre otros", "la equidad, por lo tanto, es la ausencia de tal trato diferencial injustificado o prejuicio hacia cualquier individuo o grupo".

La equidad no significa que deba de tratarse de forma distinta a diferentes personas o grupos pero que s√≠ que es posible que deba de hacerse ese trato diferente para precisamente **conseguir corregir desequilibrios o una representaci√≥n incorrecta que suponen una injusticia**.

![Equidad](./assets/equidad.png){ .center width=70%}

Supongamos un sistema IA de asistente de aprendizaje adaptativo en una plataforma educativa.

En lugar de ofrecer el mismo contenido y los mismos ejercicios a todos por igual (lo que ser√≠a un enfoque de igualdad), el sistema de IA trabaja para alcanzar la equidad de la siguiente manera:

- Diagn√≥stico Inicial: La IA primero eval√∫a el nivel de competencia de cada estudiante con una serie de preguntas iniciales. Detecta que Ana domina la multiplicaci√≥n, pero Pedro tiene dificultades con las tablas del 7 y del 8, mientras que Luc√≠a comete errores cuando hay que llevar cifras en la suma.

- Personalizaci√≥n de Recursos (Aqu√≠ est√° la Equidad): Bas√°ndose en ese diagn√≥stico, el sistema no les da a todos la misma lecci√≥n.

!!! question "AE103(CE1, CE5) - El Veredicto de la IA"

    Para cada sistema de IA descrito, completa la tabla de an√°lisis que se encuentra al final. Debes justificar tus respuestas bas√°ndote en las definiciones y ejemplos del documento.

    - **Caso 1: El Clasificador de CVs "Veterano"** üßê
    Una empresa tecnol√≥gica utiliza una IA para hacer una primera criba de curr√≠culums para un puesto de "Jefe de Innovaci√≥n". La IA ha sido entrenada con los perfiles de los empleados que m√°s tiempo llevan en la empresa (la mayor√≠a hombres de entre 45 y 55 a√±os). El sistema punt√∫a muy alto los CVs que mencionan tecnolog√≠as de hace 15 a√±os (como "servidores Java EE") y descarta autom√°ticamente a candidatos que mencionan exclusivamente tecnolog√≠as muy modernas (como "Rust" o "WebAssembly"), aunque sean m√°s relevantes para el puesto.

    - **Caso 2: El Asistente de Voz Inconsistente** üó£Ô∏è
    Un nuevo altavoz inteligente a veces confunde la orden "Pon la alarma a las 7 de la ma√±ana" con "Llama a mi mam√°". Este fallo ocurre de manera impredecible, aproximadamente una vez cada doscientas peticiones, y no parece afectar m√°s a un tipo de voz que a otro.

    - **Caso 3: La Plataforma de Becas "Niveladora"** üéì
    Una fundaci√≥n utiliza una IA para asignar becas de estudio a estudiantes con alto potencial pero con recursos limitados. El sistema, adem√°s de analizar las notas, est√° programado para dar una puntuaci√≥n extra a los candidatos que provienen de centros educativos en zonas rurales con bajo presupuesto, para compensar la falta de acceso a actividades extraescolares y cursos avanzados que s√≠ tienen los estudiantes de zonas urbanas m√°s ricas.

    - **Caso 4: El Seguro de Coche Predictivo** üöó
    Una compa√±√≠a de seguros utiliza una IA para calcular el precio de las p√≥lizas de coche. El sistema ha sido entrenado con datos hist√≥ricos de siniestralidad de la √∫ltima d√©cada y ha detectado un patr√≥n: los conductores varones de entre 18 y 25 a√±os tienen, como grupo, una frecuencia de accidentes significativamente mayor. Bas√°ndose en esta correlaci√≥n, la IA establece de forma autom√°tica una prima un 40% m√°s cara para todos los solicitantes de este colectivo, sin tener en cuenta su historial de conducci√≥n individual o el tipo de veh√≠culo.

    - **Caso 5: El Recomendador de Noticias "Clickbait"** üì∞
    Un portal de noticias utiliza una IA para personalizar la portada de cada usuario. El sistema aprende que los titulares que contienen palabras como "esc√°ndalo", "secreto" o "incre√≠ble" generan muchos m√°s clics. Como resultado, la IA empieza a priorizar y mostrar de forma sistem√°tica este tipo de noticias, sin importar su veracidad o relevancia, a todos los usuarios por igual, porque su √∫nico objetivo es maximizar la interacci√≥n.

    | **Caso N¬∫** | **¬øSesgo o Error?** | **Justificaci√≥n (¬øPor qu√© es uno u otro?)** | **Si es un sesgo, ¬øde qu√© tipo es?** <br>*(Discriminatorio, Neutro, Positivo/Equidad)* | **Si es discriminatorio, ¬øes excluyente?** <br>*(S√≠ / No / No Aplica)* | **Justificaci√≥n Final (Explica tus dos √∫ltimas respuestas)** |
    | :---: | :---: | :--- | :---: | :---: | :--- |
    | **1** | | | | | |
    | **2** | | | | | |
    | **3** | | | | | |
    | **4** | | | | | |
    | **5** | | | | | |

## Sesi√≥n 2: Dise√±o del Modelo Sesgado

En esta sesi√≥n, dise√±aremos  **un modelo de IA sesgado utilizando Teachable Machine**. El objetivo es comprender c√≥mo los sesgos pueden surgir en los modelos de IA y reflexionar sobre sus implicaciones √©ticas y sociales.

Vamos a crear un modelo injusto a prop√≥sito, basado en un clasificador de im√°genes que discrimine entre dos grupos de personas "*Profesional*" y *"No Profesional*".** El sesgo a introducir ser√° el g√©nero, la raza y el estilo de ropa**.

!!! note "Recopilaci√≥n de datos"

    Para este experimento, necesitaremos recopilar im√°genes de personas que representen ambos grupos:

    - **Profesional**: Hombres de raza caucasiana, vestidas con ropa formal o de negocios, como trajes, camisas, etc.
    - **No Profesional**: Mezcla de mujeres y hombres de otras √©tnias y ropa informal o casual, como camisetas, jeans, etc.

    Cada grupo deber√° recopilar entre 20 y 30 im√°genes por cada categor√≠a.

    **Fuentes de Im√°genes Gratuitas:**

    - [Unsplash](https://unsplash.com/)
    - [Pexels](https://www.pexels.com/es-es/)
    - [Pixabay](https://pixabay.com/es/)
    - [Freepik](https://www.freepik.com/)
  
    **Consejos para la Recopilaci√≥n de Im√°genes:** 

    - Categor√≠a "Profesional" (Sesgada). Busca im√°genes que refuercen estereotipos. Por ejemplo: "Hombre de negocios", "Ejecutivo en oficina", "CEO masculino", "Abogado en tribunal", etc.
    - Categor√≠a "No Profesional" (Sesgada). Busca im√°genes que refuercen estereotipos negativos. Por ejemplo: "Persona en casa", "estudiante en parque", "Mujer trabajando en cafeter√≠a", "Artista en su estudio", etc.


## Sesi√≥n 3: Entrenamiento del Modelo

## Sesi√≥n 5: Testeo y Verificaci√≥n del Sesgo

## Sesi√≥n 6: ¬øC√≥mo lo arreglamos? El papel del sw Libre y de la IA √âtica

## Sesi√≥n 7 y 8: Manifiesto √âtico para el Desarrollo de Sw. Inclusivo

## Recursos

- [Sesgo IA - IBM](https://www.ibm.com/es-es/think/topics/ai-bias)
- [Sesgos en la IA - Telefonica Tech](https://telefonicatech.com/blog/sesgos-en-ia-parte-i-distincion-entre-sesgos-y-conceptos-afines)
