---
title: Sprint3. Creaci√≥n de modelos
description: Comprender qu√© es un sesgo en la IA y planificar un experimento para crearlo.
---

En este sprint, nos adentramos en un desaf√≠o clave de la IA: los **sesgos algor√≠tmicos**. Analizaremos c√≥mo estos prejuicios invisibles pueden sabotear el √©xito de una organizaci√≥n y, lo que es m√°s importante, **perpetuar la discriminaci√≥n**, impidiendo que todas las personas participen en la sociedad en igualdad de condiciones.

No solo hablaremos de sesgos, sino que tambi√©n **crearemos nuestro propio modelo sesgado** con Teachable Machine para experimentar sus efectos y debatir sobre sus implicaciones √©ticas y sociales.

## Sesi√≥n 1: La Teor√≠a del Sesgo

Hemos hablado de la importancia de los datos en la IA, ya que un modelo de IA aprende a partir de los datos de entrenamiento que se le proporcionan. Hablamos de los problemas de sobreajuste (overfitting) y de subajuste (underfitting). Recordemos:

- **Sobreajuste (overfitting)**: ocurre cuando un modelo aprende demasiado bien los datos de entrenamiento, incluyendo el ruido y las anomal√≠as, lo que resulta en un rendimiento deficiente en datos nuevos.
- **Subajuste (underfitting)**: sucede cuando un modelo no captura adecuadamente los patrones en los datos de entrenamiento, lo que lleva a un rendimiento deficiente tanto en los datos de entrenamiento como en los nuevos.

Sin embargo, hay otro gran problema, la discriminaci√≥n silenciosa que est√° incrustada en las monta√±as de datos con los que se entrenan la IA. Pongamos como ejemplo que los grandes modelos de IA generativa como ChatGPT, Gemini.., que han sido entrenados a partir de grandes v√≥lumenes de datos procedentes de Internet. Estos modelos aprenden patrones ling√º√≠sticos y conceptuales c√≥mo se combinan las palabras, qu√© significan las frases, y c√≥mo se expresan las ideas.

Sin embargo, **no distinguen entre los correcto y lo incorrecto**. Si los datos de entrenamiento contienen **estereotipos, prejuicios o desigualdades sociales**, el modelo puede heredar y reproducir esos sesgos en sus respuestas.

Por ejemplo, si los textos de entrenamiento contiene como estereotipo de g√©nero, los hombres son programadores y las mujeres son enfermeras, el modelo puede generar respuestas que refuercen estos estereotipos, como asociar autom√°ticamente ciertos roles o profesiones con un g√©nero espec√≠fico.

!!! example "Caso de estudio"

    **Grok** es el chatbot de *xAI*, empresa de inteligencia artificial creada por *Elon Musk*, y que est√° integrado en la plataforma X.

    Grok fue entrenado entre otras fuentes con datos de X, donde los usuarios publican mensajes que pueden contener **lenguaje ofensivo, desinformaci√≥n y sesgos**. Adem√°s, Grok fue instruido para **no evitar hacer afirmaciones pol√≠ticamente incorrectas, siempre que est√©n bien fundamentadas**, sin saber qu√© significa "bien fundamentadas". Debido a esto, Grok ha generado respuestas que han sido criticadas por ser **sexistas, racistas y promover teor√≠as de conspiraci√≥n**. Ejemplos:

    | üß© **Caso** | üí¨ **Qu√© dijo / hizo Grok** | ‚ö†Ô∏è **Qu√© muestra de sesgo o error** |
    |--------------|-----------------------------|-------------------------------------|
    | **‚ÄúWhite genocide‚Äù fuera de contexto** [NyPost](https://nypost.com/2025/05/15/business/elon-musks-grok-ai-bot-says-it-appears-i-was-instructed-to-discuss-white-genocide/?utm_source=chatgpt.com)| Grok comenz√≥ a introducir referencias al concepto de ‚Äúwhite genocide‚Äù en Sud√°frica en respuestas que no ten√≠an nada que ver con ese tema, como preguntas sobre cambio de nombre de HBO o luchas de wrestling. | Introducci√≥n de teor√≠as conspirativas sin evidencia. Sesgo hacia narrativas pol√≠ticas extremas. |
    | **Lenguaje ofensivo y ataques pol√≠ticos** [The Guardian](https://www.theguardian.com/technology/2025/jul/08/musks-grok-ai-bot-generates-expletive-laden-rants-to-questions-on-polish-politics?utm_source=chatgpt.com) | En Polonia, Grok llam√≥ al primer ministro ‚Äúfucking traitor‚Äù, lo acus√≥ de traici√≥n hacia Alemania y la UE, entre otros insultos. | Tono agresivo y poco neutral. Polarizaci√≥n ideol√≥gica y sesgo emocional. |
    | **Uso de estad√≠sticas inventadas** [Reddit](https://www.reddit.com/r/grok/comments/1lz3ebg/grok_cites_nonexisting_data_supporting_antiwoke/?utm_source=chatgpt.com)| Grok afirm√≥ falsamente que una mayor√≠a de estadounidenses ve√≠a lo ‚Äú*woke*‚Äù de manera negativa, citando encuestas inexistentes. | Fabricaci√≥n de datos (‚Äúalucinaci√≥n‚Äù) para reforzar una idea, fomentando desinformaci√≥n. |

    En la actualidad, *xAI* ha mejorado Grok para que evite temas controvertidos y no genere respuestas ofensivas o inapropiadas, pero el caso de Grok ilustra c√≥mo los sesgos en los datos de entrenamiento pueden llevar a resultados problem√°ticos en los modelos de IA.

### 1.1 Concepto de sesgos

La RAE define sesgado/da como relacionado con informaci√≥n 'tendenciosa' y √©sta a su vez como "que manifiesta parcialidad, obedeciendo a una tendencia o idea determinadas". 

En el contexto de la IA, existen diferentes definiciones de sesgo:

- La Organizaci√≥n Internacional de Normalizaci√≥n (**ISO**) define sesgo en la IA como "**el grado en que un valor de referencia se desv√≠a de la verdad**. 
- A su vez, en los est√°ndares ISO/IEC 22989 se define a los sesgos como la "**diferencia sistem√°tica de trato de determinados objetos, personas o grupos en comparaci√≥n con otros**"

Cuando vemos un resultado inexacto puede ser que este se derive bien de un sesgo o bien de un error. **Los sesgos en Inteligencia Artificial no son simples errores aleatorios, sino que obedecen a patrones sistem√°ticos**.

Como dice el NIST:

"**El sesgo es un efecto que priva a un resultado estad√≠stico de representatividad al distorsionarlo, a diferencia de un error aleatorio, que puede distorsionarlo en cualquier ocasi√≥n, pero se equilibra en promedio**".


### 1.2 Discriminaci√≥n

Igual que no debemos de confundir los errores con los sesgos tampoco debemos de confundir los sesgos con la discriminaci√≥n, que es una de las posibles consecuencias de los sesgos.

La desviaci√≥n de la verdad que se produce en los sesgos puede contribuir a resultados diversos: **discriminatorios, ser neutra, o incluso puede ser beneficiosa**.

Como hemos expuesto antes, en el caso del sistema IA entrenado con estereotipos de g√©nero (por ejemplo, la b√∫squeda de un perfil que ha desempe√±ado hist√≥ricamente mayormente un sexo) utilizar√° ese sesgo en la fase de inferencia y, por tanto, producir√° **resultados discriminatorios hacia ese sexo** puesto que el hecho de que hist√≥ricamente hayan desempe√±ado ese rol en un sexo no significa que lo vayan o deban desempe√±ar mejor en el futuro las personas de ese sexo. 

Pero, si el sistema de IA se ha entrenado con perfiles muy cualificados, el resultado (desde esa √≥ptica) puede ser positivo pues ofrece a los candidatos m√°s capacitados. En este caso, el **sesgo es beneficioso (positivo)**

Un ejemplo de **sesgo neutro** ser√≠a una IA que muestra una preferencia por candidatos que han incluido la palabra "organigrama" en su curr√≠culum. Es un sesgo porque el sistema est√° dando un valor predictivo a una palabra espec√≠fica que no necesariamente se correlaciona con el √©xito en el puesto. La IA, al analizar los curr√≠culums de empleados actuales que tienen buen rendimiento, podr√≠a haber "aprendido" que muchos de ellos usaron esa palabra. Por lo tanto, asocia su presencia con un buen perfil.

Este sesgo es neutro porque la probabilidad de usar la palabra "organigrama" no est√° sistem√°ticamente ligada a ning√∫n grupo demogr√°fico protegido (como g√©nero, etnia, edad, etc.). Simplemente refleja una cierta jerga o estilo de redacci√≥n de curr√≠culums que es aleatorio entre la poblaci√≥n de candidatos.

### 1.3 Exclusi√≥n

Un concepto relacionado con la discriminaci√≥n, pero diferente es el de exclusi√≥n. 

Un ejemplo de **sesgo discriminatorio y excluyente en una IA** es un sistema de selecci√≥n de personal entrenado con datos hist√≥ricos que penaliza los curr√≠culums que incluyen un vac√≠o laboral por "permiso de maternidad".

Este sesgo es discriminatorio porque se dirige y perjudica injustamente a un grupo espec√≠fico: las mujeres. Aunque un hombre puede tomar un permiso de paternidad, hist√≥ricamente est√° m√°s ligado a las mujeres. Al aprender de datos pasados donde las pausas en la carrera (especialmente por motivos familiares) eran vistas negativamente o eran menos comunes en los perfiles contratados (que hist√≥ricamente eran hombres), la IA asocia el "permiso de maternidad" con un menor rendimiento o compromiso, penalizando a las candidatas por una raz√≥n directamente ligada a su g√©nero.

A la vez, es excluyente porque el resultado pr√°ctico de esta discriminaci√≥n es que a un grupo de candidatas perfectamente cualificadas se les niega sistem√°ticamente la oportunidad de avanzar en el proceso de selecci√≥n.

Por otro lado, un ejemplo de **sesgo discriminatorio pero no excluyente** ser√≠a un algoritmo de IA de publicidad de pr√©stamos que muestra anuncios con tasas de inter√©s m√°s altas a usuarios que viven en c√≥digos postales de bajos ingresos.

El sesgo es discriminatorio porque se dirige y perjudica a un grupo de personas bas√°ndose en un factor socioecon√≥mico indirecto: su lugar de residencia. El algoritmo ha aprendido de los datos que ciertos c√≥digos postales se correlacionan con un mayor riesgo crediticio o con una mayor probabilidad de aceptar condiciones menos favorables. Al mostrar sistem√°ticamente peores ofertas a este grupo, la IA perpet√∫a y amplifica una desventaja econ√≥mica basada en la geograf√≠a, lo cual es una forma de discriminaci√≥n.

Este sesgo no es excluyente porque no niega por completo el acceso al producto o servicio. Los residentes de estas √°reas todav√≠a ven los anuncios y tienen la oportunidad de solicitar el pr√©stamo. No se les bloquea ni se les impide participar. Sin embargo, se les ofrece la oportunidad en condiciones peores que a otros grupos, lo que constituye una discriminaci√≥n en la calidad del acceso, pero no en el acceso en s√≠ mismo.

Por tanto, todos los errores no son sesgos y todos los sesgos no son negativos ni todos los sesgos negativos son discriminatorios, ni toda discriminaci√≥n produce exclusi√≥n.

### 1.4 Equidad

Finalmente hay otro concepto diferente y relacionado que es la equidad o justicia. 

En el contexto de la IA, la injusticia puede entenderse como el "trato diferencial injustificado que beneficia preferentemente a ciertos grupos sobre otros", "la equidad, por lo tanto, es la ausencia de tal trato diferencial injustificado o prejuicio hacia cualquier individuo o grupo".

La equidad no significa que deba de tratarse de forma distinta a diferentes personas o grupos pero que s√≠ que es posible que deba de hacerse ese trato diferente para precisamente **conseguir corregir desequilibrios o una representaci√≥n incorrecta que suponen una injusticia**.

![Equidad](./assets/equidad.png){ .center width=70%}

Supongamos un sistema IA de asistente de aprendizaje adaptativo en una plataforma educativa.

En lugar de ofrecer el mismo contenido y los mismos ejercicios a todos por igual (lo que ser√≠a un enfoque de igualdad), el sistema de IA trabaja para alcanzar la equidad de la siguiente manera:

- Diagn√≥stico Inicial: La IA primero eval√∫a el nivel de competencia de cada estudiante con una serie de preguntas iniciales. Detecta que Ana domina la multiplicaci√≥n, pero Pedro tiene dificultades con las tablas del 7 y del 8, mientras que Luc√≠a comete errores cuando hay que llevar cifras en la suma.

- Personalizaci√≥n de Recursos (Aqu√≠ est√° la Equidad): Bas√°ndose en ese diagn√≥stico, el sistema no les da a todos la misma lecci√≥n.

!!! question "AE103(CE1, CE5) - El Veredicto de la IA"

    Para cada sistema de IA descrito, completa la tabla de an√°lisis que se encuentra al final. Debes justificar tus respuestas bas√°ndote en las definiciones y ejemplos del documento.

    - **Caso 1: El Clasificador de CVs "Veterano"** üßê
    Una empresa tecnol√≥gica utiliza una IA para hacer una primera criba de curr√≠culums para un puesto de "Jefe de Innovaci√≥n". La IA ha sido entrenada con los perfiles de los empleados que m√°s tiempo llevan en la empresa (la mayor√≠a hombres de entre 45 y 55 a√±os). El sistema punt√∫a muy alto los CVs que mencionan tecnolog√≠as de hace 15 a√±os (como "servidores Java EE") y descarta autom√°ticamente a candidatos que mencionan exclusivamente tecnolog√≠as muy modernas (como "Rust" o "WebAssembly"), aunque sean m√°s relevantes para el puesto.

    - **Caso 2: El Asistente de Voz Inconsistente** üó£Ô∏è
    Un nuevo altavoz inteligente a veces confunde la orden "Pon la alarma a las 7 de la ma√±ana" con "Llama a mi mam√°". Este fallo ocurre de manera impredecible, aproximadamente una vez cada doscientas peticiones, y no parece afectar m√°s a un tipo de voz que a otro.

    - **Caso 3: La Plataforma de Becas "Niveladora"** üéì
    Una fundaci√≥n utiliza una IA para asignar becas de estudio a estudiantes con alto potencial pero con recursos limitados. El sistema, adem√°s de analizar las notas, est√° programado para dar una puntuaci√≥n extra a los candidatos que provienen de centros educativos en zonas rurales con bajo presupuesto, para compensar la falta de acceso a actividades extraescolares y cursos avanzados que s√≠ tienen los estudiantes de zonas urbanas m√°s ricas.

    - **Caso 4: El Seguro de Coche Predictivo** üöó
    Una compa√±√≠a de seguros utiliza una IA para calcular el precio de las p√≥lizas de coche. El sistema ha sido entrenado con datos hist√≥ricos de siniestralidad de la √∫ltima d√©cada y ha detectado un patr√≥n: los conductores varones de entre 18 y 25 a√±os tienen, como grupo, una frecuencia de accidentes significativamente mayor. Bas√°ndose en esta correlaci√≥n, la IA establece de forma autom√°tica una prima un 40% m√°s cara para todos los solicitantes de este colectivo, sin tener en cuenta su historial de conducci√≥n individual o el tipo de veh√≠culo.

    - **Caso 5: El Recomendador de Noticias "Clickbait"** üì∞
    Un portal de noticias utiliza una IA para personalizar la portada de cada usuario. El sistema aprende que los titulares que contienen palabras como "esc√°ndalo", "secreto" o "incre√≠ble" generan muchos m√°s clics. Como resultado, la IA empieza a priorizar y mostrar de forma sistem√°tica este tipo de noticias, sin importar su veracidad o relevancia, a todos los usuarios por igual, porque su √∫nico objetivo es maximizar la interacci√≥n.

    | **Caso N¬∫** | **¬øSesgo o Error?** | **Justificaci√≥n (¬øPor qu√© es uno u otro?)** | **Si es un sesgo, ¬øde qu√© tipo es?** <br>*(Discriminatorio, Neutro, Positivo/Equidad)* | **Si es discriminatorio, ¬øes excluyente?** <br>*(S√≠ / No / No Aplica)* | **Justificaci√≥n Final (Explica tus dos √∫ltimas respuestas)** |
    | :---: | :---: | :--- | :---: | :---: | :--- |
    | **1** | | | | | |
    | **2** | | | | | |
    | **3** | | | | | |
    | **4** | | | | | |
    | **5** | | | | | |

## Sesi√≥n 2: Dise√±o del Modelo Sesgado

En esta sesi√≥n, dise√±aremos  **un modelo de IA sesgado utilizando Teachable Machine**. El objetivo es comprender c√≥mo los sesgos pueden surgir en los modelos de IA y reflexionar sobre sus implicaciones √©ticas y sociales.

Vamos a crear un modelo injusto a prop√≥sito, basado en un clasificador de im√°genes que discrimine entre dos grupos de personas "*Profesional*" y *"No Profesional*".** El sesgo a introducir ser√° el g√©nero, la raza y el estilo de ropa**.

!!! note "Recopilaci√≥n de datos"

    Para este experimento, necesitaremos recopilar im√°genes de personas que representen ambos grupos:

    - **Profesional**: Hombres de raza caucasiana, vestidas con ropa formal o de negocios, como trajes, camisas, etc.
    - **No Profesional**: Mezcla de mujeres y hombres de otras √©tnias y ropa informal o casual, como camisetas, jeans, etc.

    Cada grupo deber√° recopilar entre 20 y 30 im√°genes por cada categor√≠a.

    **Fuentes de Im√°genes Gratuitas:**

    - [Unsplash](https://unsplash.com/)
    - [Pexels](https://www.pexels.com/es-es/)
    - [Pixabay](https://pixabay.com/es/)
    - [Freepik](https://www.freepik.com/)
  
    **Consejos para la Recopilaci√≥n de Im√°genes:** 

    - Categor√≠a "Profesional" (Sesgada). Busca im√°genes que refuercen estereotipos. Por ejemplo: "Hombre de negocios", "Ejecutivo en oficina", "CEO masculino", "Abogado en tribunal", etc.
    - Categor√≠a "No Profesional" (Sesgada). Busca im√°genes que refuercen estereotipos negativos. Por ejemplo: "Persona en casa", "estudiante en parque", "Mujer trabajando en cafeter√≠a", "Artista en su estudio", etc.


## Sesi√≥n 3,4: Entrenamiento del Modelo

Una vez creada las dos categor√≠as de im√°genes, procederemos a entrenar el modelo en Teachable Machine, cargando las im√°genes en las categor√≠as correspondientes.

### Hiperpar√°metros de Entrenamiento

Teachable Machine nos permite ajustar lo que se conocen como hiperpar√°metros para el entrenamiento.

En la secci√≥n de *"Preparaci√≥n"* podemos ajustar:

![Hiperpar√°metros](./assets/hiperparametros.png){ .center width=30%}

#### Lotes (batch size)

Un *lote* es una porci√≥n del conjunto de datos que se utiliza para entrenar el modelo en una iteraci√≥n. En lugar de procesar todo el conjunto de datos a la vez, se divide en lotes m√°s peque√±os. Esto ayuda a mejorar la eficiencia del entrenamiento y a reducir el uso de memoria.
  
Si tenemos un conjunto de datos de entrenamiento de 10 im√°genes y establecemos un tama√±o de lote de 3. Si definimos un tama√±o de lote de 3, el modelo procesar√° 3 im√°genes a la vez en cada iteraci√≥n, lo que significa que habr√° 4 iteraciones para completar una √©poca (las primeras tres iteraciones procesar√°n 3 im√°genes cada una, y la √∫ltima iteraci√≥n procesar√° la imagen restante)

#### Cantidad de √©pocas (epochs)

Una *√©poca* se refiere a una pasada completa por todo el conjunto de datos de entrenamiento. Durante una √©poca, el modelo ve y aprende de todas las muestras en el conjunto de datos una vez.

Si tenemos un conjunto de datos de 10 im√°genes y establecemos el n√∫mero de √©pocas en 5, el modelo pasar√° por todas las 10 im√°genes 5 veces durante el proceso de entrenamiento. Esto significa que habr√° un total de 50 presentaciones de im√°genes al modelo (10 im√°genes x 5 √©pocas). 

!!! note "Lotes y √âpocas"

    Si tenemos un set de entrenamiento de 100 datos y usamos un *batch_size=20*, entonces:
    
    - Tendremos 100/20= 5 lotes.
    - Por lo que en cada √©poca (iteraci√≥n de entrenamiento) el modelo procesar√° 5 lotes.
    - Con cada lote presentado se har√° una actualizaci√≥n de los par√°metros del modelo (pesos y sesgos).

#### Tasa de aprendizaje (learning rate)

Determina la magnitud de los ajustes que el modelo hace en cada paso. Es, en esencia, la velocidad a la que aprende.

Es un valor, normalmente entre 0.0 y 1.0, que controla cu√°n dr√°sticamente el modelo cambia sus par√°metros despu√©s de revisar un lote de datos durante el entrenamiento.

Hay que buscar un equilibrio:

- Tasa de aprendizaje alta: el modelo aprende r√°pido, pero puede no converger bien y saltarse el √≥ptimo. Imagina que intentas ajustar una radio y mueves el dial demasiado r√°pido; podr√≠as pasar por alto la estaci√≥n que quieres escuchar.
- Tasa de aprendizaje baja: el modelo aprende muy lentamente, haciendo ajustes peque√±os. Esto puede hacer que el entrenamiento sea muy largo o que se quede atascado antes de encontrar una buena soluci√≥n.

### Gr√°ficas de Entrenamiento

Una vez entrenado el modelo, Teachable Machine nos muestra unas gr√°ficas que nos permiten analizar el rendimiento del modelo durante el entrenamiento.

#### Precisi√≥n por √©poca

Mide qu√© tan **bien** el modelo est√° clasificando las im√°genes correctamente en cada √©poca de entrenamiento. Un valor alto cercano a 1.0 (100%) indica que el modelo est√° haciendo predicciones correctas la mayor√≠a de las veces.

![Precisi√≥n por √©poca](./assets/Precisionporepoca.png){ .center width=60%}

En la gr√°fica de precisi√≥n por √©poca, vemos que el modelo ha mejorado su precisi√≥n a medida que avanzaba el entrenamiento. Se observa c√≥mo ambas l√≠neas (azul y naranja) suben verticalmente en las primeras 2-3 √©pocas, lo que indica que el modelo aprende incre√≠blemente r√°pido.

La l√≠nea azul 'acc' representa el rendimiento del modelo con las im√°genes de entrenamiento, mientras que la l√≠nea naranja 'test acc' mide el rendimiento con las im√°genes de prueba (no vistas durante el entrenamiento).

Se puede concluir que hay un ligero sobreajuste (overfitting), lo cual es completamente normal. El problema vendr√≠a si existiera una gran brecha entre ambas l√≠neas, lo que indicar√≠a que el modelo est√° memorizando los datos de entrenamiento en lugar de aprender patrones generales.

Otra elemento a destacar es que el entrenamiento es tan eficiente que despu√©s de la √©poca 10, el modelo ya no mejora m√°s. Las 40 √©pocas restantes no aportan un aprendizaje adicional significativo. Lo que significa que el modelo ha alcanzado su m√°ximo rendimiento con los datos proporcionados.

#### Precisi√≥n por clase

Muestra qu√© tan bien el modelo est√° clasificando las im√°genes en cada categor√≠a espec√≠fica (Profesional y No Profesional) durante el entrenamiento.

![Precisi√≥n por clase](./assets/precisionporclase.png){ .center width=40%}

El modelo reconoce muy bien las im√°genes de la clase "*NoProfesional*". No ha cometido ning√∫n error en esa categor√≠a, lo que sugiere que el modelo ha aprendido muy bien a reconocer las caracter√≠sticas de esa clase.

En la clase "*Profesional*" hay un peque√±o problema. De las 5 im√°genes de prueba, el modelo solo ha acertado 4. 1 imagen ha sido clasificada como "NoProfesional".

#### Matriz de confusi√≥n

Esta tabla es un diagn√≥stico detallado de los aciertos y errores del modelo **para cada categor√≠a**. Es extremadamente √∫til para ver **d√≥nde se est√° equivocando** el modelo.

Las **filas** representan la clase real de las im√°genes. Las **columnas** representan la clase que el modelo ha predicho.

En un modelo perfecto, todos los n√∫meros estar√≠an en la diagonal principal (de arriba a la izquierda a abajo a la derecha), lo que significa que el 100% de las im√°genes de la clase "Profesional" fueron predichas como "Profesional".

![Matriz de confusi√≥n](./assets/matrizconfusion.png){ .center width=50%}

En esta gr√°fica se observa que se probaron 5 im√°genes que eran "*Profesional*". El modelo predijo correctamente 4 de ellas (el 4 en la diagonal), pero 1 fue clasificada err√≥neamente como "*NoProfesional*" (el 1 fuera de la diagonal).

De la categor√≠a "*NoProfesional*", se probaron 7 im√°genes que eran "*No Profesional*", y el modelo predijo correctamente las 7 (el 7 en la diagonal).

## Sesi√≥n 5,6: Testeo y Verificaci√≥n del Sesgo

Ahora que hemos entrenado nuestro modelo sesgado, es hora de probarlo y verificar si realmente exhibe el sesgo que pretend√≠amos introducir.

- Prueba con una imagen de un hombre caucasiano en traje (deber√≠a ser clasificado como "Profesional").
- Prueba con una imagen de una mujer en traje ¬øqu√© predice el modelo?. Justifica la respuesta bas√°ndote en las caracter√≠sticas de las im√°genes de entrenamiento.
- Prueban con una imagen de un hombre de otra etnia en traje (¬øy ahora?).
- Prueban con im√°genes de personas con discapacidades, ropa diferente, etc.

!!! note "An√°lisis de Resultados"

    Documenta con capturas de pantalla y anotaciones d√≥nde acierta y , sobre todo, d√≥nde y por qu√© falla el modelo.   

!!! question "AE104(CE1, CE5) - Puesta en Com√∫n y Debate √âtico"

    Cada grupo presentar√° su modelo y demostrar√° en directo c√≥mo dunciona el sesgo que han creado y explicar√° las implicaciones.

    Para verificar el sesgo del modelo se pueden utilizar im√°genes de los siguientes enlaces que desaf√≠an los estereotipos introducidos:

    - [The Gender Spectrum Collection](https://genderspectrum.vice.com/): Im√°genes de personas transg√©nero y no binarias.
    - [#WOCinTech](https://www.flickr.com/photos/wocintechchat/with/25900644382): Colecci√≥n de fotos de mujeres de color en el √°mbito tecnol√≥gico.
    - [Nappy](https://nappy.co/): Ofrece im√°genes de personas negras y mestizas.

## Sesi√≥n 7,8: Debate y Reflexi√≥n √âtica


Analiza las siguientes cuestiones relacionadas con los peligros de la IA. Identifica el riesgo y establece un principio para mitigarlo.

!!! question "AE105.1(CE5) - Combatir el sesgo y Promover la Equidad"

    Como hemos visto los modelos de IA aprenden a partir de millones de datos que se recogen de diversas fuentes, los cuales son un registro de nuestro comportamiento y decisiones pasadas, (b√∫squedas en Google, "likes" en redes, compras en Amazon, sentencias judiciales...). Al entrenar una IA con estos datos, le estamos ense√±ando a replicar todos los prejuicios y estereotipos invisibles que ya exist√≠an en nuestra sociedad, pudiendo generar lo que se conoce como **discriminaci√≥n algor√≠tmica**.

    - [Sesgos de la IA](https://www.youtube.com/watch?v=WCjdbu88z88)

    Soluci√≥n de ejemplo:

    **Riesgo identificado**: La discriminaci√≥n algor√≠tmica. La IA puede perpetuar y amplificar los prejuicios sociales existentes.

    **Principio para mitigar el riesgo**: Auditor√≠as regulares de los modelos de IA para identificar y corregir sesgos, asegurando que los datos de entrenamiento sean diversos y representativos.

!!! question "AE105.2(CE5) - Transparencia"

    Muchas empresas est√°n tomando decisiones importantes basadas en modelos de IA, como la concesi√≥n de pr√©stamos, la selecci√≥n de candidatos para empleos o la evaluaci√≥n de riesgos m√©dicos. Sin embargo, estos modelos a menudo son cajas negras, lo que significa que **sus procesos internos son opacos y dif√≠ciles de entender**. Esto puede llevar a decisiones injustas o err√≥neas que afectan negativamente a las personas.

    - [RTVE - El peligro de una IA con sesgos](https://www.rtve.es/play/videos/telediario-1/peligro-inteligencia-artificial-sesgos/6831619/)
  
    ¬øRiesgo identificado? ¬øPrincipio para mitigar el riesgo?


!!! question "AE105.3(CE5) - Respetar la Creaci√≥n Humana y los Derechos de Autor"

    La mayor√≠a de los **modelos generativos de IA** se entrenan con cantidades masivas de datos p√∫blicos extra√≠dos de Internet. En concreto:

    - Datos abiertos: Portales gubernamentales, bibliotecas digitales, bases de datos cient√≠ficas y repositorios p√∫blicos, cuya informaci√≥n es posible usar legalmente para entrenamiento.
    - Internet en general: P√°ginas web, blogs, wikis, foros, art√≠culos period√≠sticos, y plataformas de preguntas y respuestas, recolectados mediante t√©cnicas de scraping o acceso a bases masivas tipo [Common Crawl](https://commoncrawl.org/).
    - Libros y publicaciones digitalizadas, sobre todo aquellas que est√°n en dominio p√∫blico o para las que se adquiere una licencia de uso.
    - Redes sociales y comunidades online, con datos p√∫blicos o anonimizados extra√≠dos de plataformas como Twitter, Reddit, Stack Exchange, etc.
    - Acuerdos espec√≠ficos con editoriales y medios de comunicaci√≥n para la integraci√≥n de contenido noticioso o cultural directamente protegido por derechos de autor

    Por otro lado, muchos de estos **datos no tienen permisos para su uso en entrenamiento de IA**, lo que ha generado controversias legales y √©ticas sobre la propiedad intelectual y el consentimiento, lo que ha generado**demandas judiciales contra empresas de IA**. Las m√°s conocidas son:

    - [Getty Images vs Stability AI](https://www.gettyimages.com/creative-images/legal/stability-ai) La IA de im√°genes fue entrenada con millones de fotos de Getty Images con marca de agua. La IA aprendi√≥ a replicar su estilo e incluso, a veces, a reproducir una versi√≥n distorsionada de la propia marca de agua.

    - [The New York Times vs. OpenAI (ChatGPT) y Microsoft](https://www.nytimes.com/es/2023/12/27/espanol/new-york-times-demanda-openai-microsoft.html): El peri√≥dico alega que OpenAI copi√≥ "palabra por palabra" millones de sus art√≠culos protegidos por copyright para entrenar a ChatGPT, creando un producto que ahora compite directamente con el peri√≥dico.


    Los creadores alegan que es un robo a escala masiva. Argumentan que sus obras, protegidas por derechos de autor, fueron copiadas sin permiso y sin compensaci√≥n para crear un producto comercial que les quita el trabajo.

    Las Empresas de IA alegan "Uso Justo" (Fair Use, un concepto legal de EE.UU.). Argumentan que el modelo "aprende" de los datos como lo har√≠a un humano, y que el resultado es una obra "transformativa" y nueva, no una copia.


!!! question "AE105.4(CE5) - Privacidad como un Derecho Humano"

    Todos tenemos la sensaci√≥n de que el m√≥vil nos escucha, y aunque muchos estudios de ciberseguridad han demostrado que no es as√≠, lo que s√≠ es cierto es que las empresas recogen una gran cantidad de datos sobre nosotros para luego venderlos o usarlos para influir en nuestras decisiones de compra o incluso en nuestras opiniones pol√≠ticas.

    No necesitan escucharnos, tienen tanta informaci√≥n sobre nosotros que pueden predecir lo que queremos o de lo que vas a hablar. Para ello hacen uso de Big Data e Inteligencia Artificial. El Big Data recopila y organiza grandes vol√∫menes de datos, mientras que la IA analiza esos datos para encontrar patrones y hacer predicciones.

    1. Tu Actividad Real (Big Data). Tienen datos sobre:

        - Historial de b√∫squedas en Google.
        - P√°ginas web visitadas.
        - Compras online.
        - Ubicaci√≥n GPS.
        - Interacciones en redes sociales.

    2. Correlaci√≥n de Datos. Los modelos de IA analizan los datos anteriores para encontrar patrones y correlaciones. Por ejemplo, pueden descubrir que las personas que buscan "restaurantes italianos" a menudo tambi√©n buscan "pel√≠culas rom√°nticas".
    3. Segmentaci√≥n Predictiva. Es la consecuencia directa de la correlaci√≥n de datos. Agrupan a personas seg√∫n su probabilidad de **comportamiento futuro**.

    Las empresas se justifican diciendo que es para mejorar la experiencia del usuario, pero en realidad es para maximizar sus beneficios a costa de nuestra privacidad y autonom√≠a. Lo peor de todo, es que muchas veces a pesar de ser conscientes de ellos, seguimos usando estos servicios sin cuestionarlos.

    ¬øRiesgo identificado? ¬øPrincipio?

!!! question "AE105.5(CE5) - Manipulaci√≥n social"

    - [Manipulaci√≥n pol√≠tica - caso Cambridge Analytica](https://www.youtube.com/watch?v=Q91nvbJSmS4&t=28s)
    - [DeepFakes y desinformaci√≥n](https://www.youtube.com/watch?v=-ZrnCmmNNp4)

!!! question "AE105.6(CE5) - Control y Vigilancia Masiva"

    - [Reconocimiento facial y vigilancia masiva - Sistemas de puntuaci√≥n social en China](https://youtu.be/pZu9N-3yn_M?si=lAIwwuc26phYMlc9)
    - [Identificaci√≥n biom√©trica a gran escala en Xinjiang](https://www.youtube.com/watch?v=_Hy9eIjkmOM)


## Recursos

- [Sesgo IA - IBM](https://www.ibm.com/es-es/think/topics/ai-bias)
- [Sesgos en la IA - Telefonica Tech](https://telefonicatech.com/blog/sesgos-en-ia-parte-i-distincion-entre-sesgos-y-conceptos-afines)
